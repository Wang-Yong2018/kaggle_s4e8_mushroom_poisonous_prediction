---
title: "ml_workflow"
author: "WangYong"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Target
it is used car price prediction, evaluation metric is rmse.
the mean price of train is 43878.02. use mean price as single model , the kaggle public/private score is 78643/70105. 

## library & load_data 
### library
```{r}
library(tidyverse)
library(tidymodels)
library(future)
library(purrr)
library(furrr)
library(textrecipes)

library(bonsai)
library(lightgbm)
library(xgboost)
library(ranger)
library(ranger)
library(readr)

library(lubridate)
```
```{r}
tmp<- vroom::vroom(file.path(data_path, 'train.csv'),n_max=10**6)
problems(tmp)
                  
```

### loading data
```{r}
data_path <- '../input/playground-series-s4e8/'
train<- 
  read.csv(file.path(data_path, 'train.csv'))|>
  slice_head(n=10**5)
#test <-  readr::read_csv(file.path(data_path, 'test.csv'))
submission <-  readr::read_csv(file.path(data_path, 'sample_submission.csv'), 
                  col_types = list(col_double(), col_logical(), col_factor()))|>
  dplyr::slice_sample(prop=0.1)
```

### quick skim
```{r}
train|> skimr::skim()
```


```{r}
test|> skimr::skim()
```
```{r}
submission |> skimr::skim()
```

### check if train & test is same distribution
```{r}
get_df_var<-function(df){
  df|>
    summarize_all(var)|>
    pivot_longer(cols=everything(),
                 names_to='feature',
                 values_to='variance')
  
}
train|>get_df_var()
```


## coding
### 1. Data Loading and Initial Exploration ----



### 2. Feature Engineering ----
- leave it in the preprocessing recipe

### 3. Data Splitting ----
#### augment_df


#### split/cv

```{r}
set.seed(1234)
df_split <- initial_split(train, prop = 0.8, strata = price)
 train_set <- training(df_split)
 test_set <- testing(df_split)
cv_folds <- vfold_cv(train_set,v = 5,strata=price)
```



### 4. Preprocessing Recipe ----

#### 4.1 v0 base_line



 
#### 4.10 all recipes
```{r}
set.seed(1234)
library(future)
library(furrr)
selected_rcps <- list(#base=rcp_bs_v0,
                      v1_engine=rcp_bs_v1,
                      #v2_transmission=rcp_bs_v2,
                      #v3_model = rcp_bs_v3,
                      #v4_eng_trans=rcp_bs_v4,
                      v5_engin_model= rcp_bs_v5)
plan(multisession,workers = 5)
selected_rcps|>future_map(\(rcp_item) rcp_item|>prep()|>bake(new_data=train)|>summary())
plan(sequential)
```


### 5. Model Specification ----
```{r}
lgbm_eng<-
   boost_tree(
     # trees = 500, # Number of trees
     # learn_rate = 0.1,
     # loss_reduction = 0.001,
     # sample_size = 0.85, # Added sample_size
     # tree_depth = tune(),
     # mtry = tune(),
     # min_n = tune()
   ) |>
   set_mode("regression")|>
   set_engine("lightgbm",
              metric='rmse', 
              # num_leaves = 30,
              num_threads = 4,
              verbose=1) 

lm_eng<-
   linear_reg() |>
   set_mode("regression")|>
   set_engine("lm") 

selected_eng <- list(linear=lm_eng,
                     lgbm=lgbm_eng)

```


### 6. Workflow ----
#### simple wflow
```{r}
set.seed(1234)
simple_wf_fit <-
  workflow() |>
  add_recipe(rcp_bs_v1) |>
  add_model(lgbm_eng)|>
  last_fit(df_split)

simple_wf_fit|>collect_metrics()
# simple_wf_fit |>
#   extract_fit_engine()|>
#   plot()

simple_wf_diag_df<- 
  simple_wf_fit|>
  collect_predictions()|>
  mutate(.residual=price-.pred)|>
  select(.row,.pred,.residual)|>
  bind_cols(test_set)|>
  relocate(all_of(c('price','.pred','.residual')),.after=1)
```
#### simple workflowset

```{r}
set.seed(1234)
library(future)
plan(multisession,workers = 12)
ctrl <- control_resamples(save_pred = TRUE, save_workflow = TRUE,verbose=TRUE)
wfs_result <-
  workflow_set(preproc = selected_rcps,
               models = selected_eng ) |>
  workflow_map(fn='fit_resamples',
               resamples =cv_folds,
               metrics = metric_set(rmse, rsq),
               control = ctrl
               )
wfs_result|>collect_metrics()  
  
plan(sequential)
```
### 7 stacking
```{r}
combined_fit <-
  stacks::stacks()|>
  stacks::add_candidates(wfs_result)|>
  stacks::blend_predictions()|>
  stacks::fit_members()

combined_fit|>
  autoplot(type = "weights")

autoplot(combined_fit)
```

### 7. Tuning Grid ----
```{r}
# cars_grid <- grid_space_filling(
#   #learn_rate(range = c(0.01, 0.1)),
#   # loss_reduction(range = c(0, 10)), #Keep if you want it.
#   #bag_fraction(range = c(0.7, 0.9)), # Specify prop = TRUE.
#   tree_depth(range = c(5, 10)),
#   finalize(mtry(range = c(2, 10)),
#            select(cars_train_set, -Price)),
#   min_n(range = c(2, 20)),
#   size = 10
# )

```


### 8. Cross-Validation ----
```{r}
# combined it with step3 data splitting
```


### 9. Tuning and Evaluation ----
```{r}
# plan(multisession,workers =2)
# cars_tune_results <- cars_workflow |>
#   tune_grid(
#     resamples = cars_folds,
#     grid = cars_grid,
#     metrics = metric_set(rmse),
#      control = control_grid(save_pred = TRUE, 
#                             verbose = TRUE,
#                             allow_par = F) # Keep predictions
#   )
#  
#  # Find best parameters
#  best_params <- cars_tune_results |>
#    select_best("rmse")
# 
#  # Finalize workflow with best parameters
#  final_workflow <- cars_workflow |>
#    finalize_workflow(best_params)
```


```{r}
# Fit the final workflow to the training data
# final_lgbm_fit <- last_fit(final_workflow,cars_split )
# final_lgbm_mod <- extract_workflow(final_lgbm_fit )
# collect_metrics(final_lgmb_mod)

# plan(sequential)

```


### 10. Evaluate on Test Set ----
```{r}
combined_test_result <- 
  test_set %>%
  bind_cols(predict(combined_fit, .))
combined_test_result|>rsq(price, .pred)
```

### 11. Prepare Submission ----
```{r}
final_model <- combined_fit#simple_wf_fit|>extract_workflow()
final_predictions <- final_model |>
   predict(new_data = test) 

 #Handle negative predictions
 final_predictions <- final_predictions |>
   mutate(.pred= ifelse(.pred< 0, 0, .pred))

 # Save submission file
 submission |>
   mutate(price=final_predictions$.pred)|>
   readr::write_csv("submission.csv")
 zip('submission.csv.zip','submission.csv')
 
```

## kaggle submission
```{r}
# submit latest submission.csv
system('kaggle competitions submit -c playground-series-s4e9 -f submission.csv.zip -m "stacking 69827/15.8% "')
Sys.sleep(15)
# get latest score 
system('kaggle competitions submissions -q -c playground-series-s4e9')

# get leader board score
#system('kaggle competitions leaderboard -s -v -c playground-series-s4e9')
```
